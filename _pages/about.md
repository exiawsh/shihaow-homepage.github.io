---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<span class='anchor' id='about-me'></span>
# Shihao Wang

**Ph.D. Student, Department of Computing, Hong Kong Polytechnic University (PolyU)**  
PQ515, PolyU | Tel: +86 18210705396
Email: 24053277r@connect.polyu.hk
## üåê [Google Scholar](https://scholar.google.com/citations?user=7TWugs4AAAAJ) | [Github](https://github.com/exiawsh)

---

## üßë‚Äçüíª Research Interests

- 3D Perception and Planning for Autonomous Vehicles and Robotics  
- Multimodal Foundation Models and Large Language Models  
- Streaming Video Understanding  
- Test-time Adaptation for Agentic Systems

---

## üèÜ Honors and Awards

- Winner, [CVPR24 Challenge on End-to-End Driving at Scale](https://opendrivelab.com/challenge2024/#end_to_end_driving_at_scale), 2024
- 2nd Place, [CVPR24 Challenge on Driving with Language](https://opendrivelab.com/challenge2024/#driving_with_language), 2024
- 1st Place, nuScenes leaderboard on camera-only 3D object tracking, 2023

---

## üéì Educations

- **Ph.D. in Computer Vision**  
  Department of Computing, Hong Kong Polytechnic University (PolyU), Hong Kong  
  *Sep 2024 ‚Äì Present*

- **M.Sc. in Vehicle Engineering**  
  Department of Mechanical Engineering, Beijing Institute of Technology (BIT), Beijing, China  
  *Sep 2021 ‚Äì Jun 2024*  
  GPA: 89.5/100

- **B.Sc. in Vehicle Engineering**  
  Department of Mechanical Engineering, Beijing Institute of Technology (BIT), Beijing, China  
  *Sep 2017 ‚Äì Jun 2021*  
  GPA: 87.5/100

---

## üìù Publications

- **Shihao Wang**, Guo Chen, De-An Huang, Zhiqi Li, Minghan Li, Guilin Liu, Jose M. Alvarez, Lei Zhang, Zhiding Yu.  
  *VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding*. arXiv:2507.13353, 2025.

- **Shihao Wang**, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, Jose M. Alvarez.  
  *OmniDrive: A holistic vision-language dataset for autonomous driving with counterfactual reasoning*. CVPR, 2025.

- GR00T Team.  
  *GR00T N1.5: An Improved Open Foundation Model for Generalist Humanoid Robots*. [NVIDIA Research Blog](https://research.nvidia.com/labs/gear/gr00t-n1_5/), 2025.

- Guo Chen, Zhiqi Li, **Shihao Wang**, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, et al.  
  *Eagle 2.5: Boosting long-context post-training for frontier vision-language models*. arXiv:2504.15271, 2025.

- Yuhui Wu, Liyi Chen, Ruibin Li, **Shihao Wang**, Chenxi Xie, Lei Zhang.  
  *InsViE-1M: Effective Instruction-based Video Editing with Elaborate Dataset Construction*. ICCV, 2025.

- Min Shi, **Shihao Wang**, Chieh-Yun Chen, Jitesh Jain, Kai Wang, Junjun Xiong, Guilin Liu, Zhiding Yu, Humphrey Shi.  
  *Slow-fast architecture for video multi-modal large language models*. arXiv:2504.01328, 2025.

- Zhenxin Li, **Shihao Wang**, Shiyi Lan, Zhiding Yu, Zuxuan Wu, Jose M. Alvarez.  
  *Hydra-next: Robust closed-loop driving with open-loop training*. arXiv:2503.12030, 2025.

- Zhiqi Li, Guo Chen, Shilong Liu, **Shihao Wang**, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al.  
  *Eagle 2: Building post-training data strategies from scratch for frontier vision-language models*. arXiv:2501.14818, 2025.

- Yingfei Liu, Zhiding Yu, Shiyi Lan, **Shihao Wang**, Rongyao Fang, Jan Kautz, Hongsheng Li, Jose M. Alvarez.  
  *StreamChat: Chatting with streaming video*. arXiv:2412.08646, 2024.

- Min Shi, Fuxiao Liu, **Shihao Wang**, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, et al.  
  *Eagle: Exploring the design space for multimodal llms with mixture of encoders*. arXiv:2408.15998, 2024.

- Zhenxin Li, Kailin Li, **Shihao Wang**, Shiyi Lan, Zhiding Yu, Yishen Ji, Ziyue Zhu, Jan Kautz, Zuxuan Wu, et al.  
  *Hydra-MDP: End-to-end multimodal planning with multi-target hydra-distillation*. arXiv:2406.06978, 2024.

- Xiaohui Jiang, Shuailin Li, Yingfei Liu, **Shihao Wang**, Fan Jia, Tiancai Wang, Lijin Han, Xiangyu Zhang.  
  *Far3D: Expanding the horizon for surround-view 3d object detection*. AAAI, 2024.

- **Shihao Wang**, Yingfei Liu, Tiancai Wang, Ying Li, Xiangyu Zhang.  
  *Exploring object-centric temporal modeling for efficient multi-view 3d object detection*. ICCV, 2023.

---

## üíº Internships

### NVIDIA, Shenzhen, China  
**Research Intern, AV Applied Research Group**  
*Oct 2023 ‚Äì Jan 2025*

- **Building Multimodal Brains**
  - Contributed to the design and scaling of [Eagle VLM family](https://arxiv.org/pdf/2408.15998) (Eagle, Eagle-2, Eagle-2.5), establishing new SOTA baselines for long-context multimodal reasoning.
  - Developed [VideoITG](https://arxiv.org/abs/2507.13353), an instruction-guided temporal grounding framework for long-video understanding, achieving SOTA results on VideoMME, MLVU, and Long-Video-Bench.
  - Core contributor to the vision-language foundation of [GR00T N1.5](https://research.nvidia.com/labs/gear/gr00t-n1_5/).

- **Perception‚ÄìPlanning Integration**
  - Contributed to [HydraMDP](https://arxiv.org/html/2406.06978v1) and [Hydra-NeXt](https://arxiv.org/html/2503.12030), end-to-end driving frameworks integrating object-centric perception with strategic counterfactual reasoning and multimodal trajectory planning.
  - Achieved **1st place** in End-to-End Driving at Scale and **2nd place** in Driving with Language at CVPR 2024 Autonomous Driving Grand Challenge.
  - Developed [OmniDrive](https://arxiv.org/abs/2405.01533) (CVPR 2025), a scalable pipeline for interpretable AV decision making.

---

### MEGVII Technology, Beijing, China  
**Research Intern, Foundation Model Group (PI: [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en))**  
*Apr 2022 ‚Äì Jul 2023*

- **Perception Foundation**
  - Developed [StreamPETR](https://arxiv.org/abs/2303.11926) (ICCV 2023), the first vision-based 3D object detector to match LiDAR performance on nuScenes.
  - Contributed to [Far3D](https://arxiv.org/abs/2308.09616) for long-range multi-view 3D detection, achieving competitive results on Argoverse 2 and nuScenes.

---